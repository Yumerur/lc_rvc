{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "071240db",
   "metadata": {},
   "source": [
    "# Change RVC model (supported v2 model only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977e9bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from hf_rvc import SynthesizerTrnMs256NSFsidConfig\n",
    "from transformers import VitsConfig\n",
    "def openModel(file):\n",
    "    from pathlib import Path\n",
    "    modelDir = Path(\"models\").joinpath(file)\n",
    "    cpt = torch.load(modelDir.joinpath(\"model.pth\"), map_location=\"cpu\", weights_only=True)\n",
    "    weight = cpt[\"weight\"]\n",
    "    modelConfig = SynthesizerTrnMs256NSFsidConfig(\n",
    "        *cpt[\"config\"]\n",
    "    )\n",
    "    output_sampling_rate = cpt[\"config\"][-1]\n",
    "    vitsConfig = VitsConfig(hidden_size=192, hidden_dropout=0.0, ffn_dim=768, num_attention_heads=2, ffn_kernel_size=3, layerdrop=0.0, window_size=10, ffn_dropout=0.0, prior_encoder_num_flows=4, prior_encoder_num_wavenet_layers=3, speaker_embedding_size=256)\n",
    "    return modelConfig, vitsConfig, weight, output_sampling_rate, modelDir.joinpath(\"model.pth\"), cpt[\"config\"], modelDir.joinpath(\"metadata.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982385ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from hf_rvc import SynthesizerTrnMs_HfVits\n",
    "def convert_rvc_to_hf_vits(rvc_state_dict: Dict[str, torch.Tensor],\n",
    "                           config: SynthesizerTrnMs256NSFsidConfig,\n",
    "                            vits_config: VitsConfig):\n",
    "    \"\"\"\n",
    "    Converts and loads weights from an RVC state_dict into the hybrid model.\n",
    "\n",
    "    Args:\n",
    "        rvc_state_dict (Dict[str, torch.Tensor]): The state_dict from the original RVC model checkpoint.\n",
    "        target_model (SynthesizerTrnMs_HfVits): An instance of the hybrid model to load weights into.\n",
    "    \"\"\"\n",
    "    new_state_dict = {}\n",
    "    n_layers = config.n_layers\n",
    "    n_flows = vits_config.prior_encoder_num_flows\n",
    "    n_wavenet_layers = vits_config.prior_encoder_num_wavenet_layers\n",
    "    target_model = SynthesizerTrnMs_HfVits(config, vits_config)\n",
    "\n",
    "    # --- Text Encoder Mapping (enc_p.encoder) ---\n",
    "    print(\"Mapping Text Encoder weights...\")\n",
    "    for i in range(n_layers):\n",
    "        new_state_dict[f'vistEncoder.layers.{i}.attention.emb_rel_k'] = rvc_state_dict[f'enc_p.encoder.attn_layers.{i}.emb_rel_k']\n",
    "        new_state_dict[f'vistEncoder.layers.{i}.attention.emb_rel_v'] = rvc_state_dict[f'enc_p.encoder.attn_layers.{i}.emb_rel_v']\n",
    "        new_state_dict[f'vistEncoder.layers.{i}.attention.q_proj.weight'] = rvc_state_dict[f'enc_p.encoder.attn_layers.{i}.conv_q.weight'].squeeze(-1)\n",
    "        new_state_dict[f'vistEncoder.layers.{i}.attention.q_proj.bias'] = rvc_state_dict[f'enc_p.encoder.attn_layers.{i}.conv_q.bias']\n",
    "        new_state_dict[f'vistEncoder.layers.{i}.attention.k_proj.weight'] = rvc_state_dict[f'enc_p.encoder.attn_layers.{i}.conv_k.weight'].squeeze(-1)\n",
    "        new_state_dict[f'vistEncoder.layers.{i}.attention.k_proj.bias'] = rvc_state_dict[f'enc_p.encoder.attn_layers.{i}.conv_k.bias']\n",
    "        new_state_dict[f'vistEncoder.layers.{i}.attention.v_proj.weight'] = rvc_state_dict[f'enc_p.encoder.attn_layers.{i}.conv_v.weight'].squeeze(-1)\n",
    "        new_state_dict[f'vistEncoder.layers.{i}.attention.v_proj.bias'] = rvc_state_dict[f'enc_p.encoder.attn_layers.{i}.conv_v.bias']\n",
    "        new_state_dict[f'vistEncoder.layers.{i}.attention.out_proj.weight'] = rvc_state_dict[f'enc_p.encoder.attn_layers.{i}.conv_o.weight'].squeeze(-1)\n",
    "        new_state_dict[f'vistEncoder.layers.{i}.attention.out_proj.bias'] = rvc_state_dict[f'enc_p.encoder.attn_layers.{i}.conv_o.bias']\n",
    "        new_state_dict[f'vistEncoder.layers.{i}.layer_norm.weight'] = rvc_state_dict[f'enc_p.encoder.norm_layers_1.{i}.gamma']\n",
    "        new_state_dict[f'vistEncoder.layers.{i}.layer_norm.bias'] = rvc_state_dict[f'enc_p.encoder.norm_layers_1.{i}.beta']\n",
    "        new_state_dict[f'vistEncoder.layers.{i}.final_layer_norm.weight'] = rvc_state_dict[f'enc_p.encoder.norm_layers_2.{i}.gamma']\n",
    "        new_state_dict[f'vistEncoder.layers.{i}.final_layer_norm.bias'] = rvc_state_dict[f'enc_p.encoder.norm_layers_2.{i}.beta']\n",
    "        new_state_dict[f'vistEncoder.layers.{i}.feed_forward.conv_1.weight'] = rvc_state_dict[f'enc_p.encoder.ffn_layers.{i}.conv_1.weight']\n",
    "        new_state_dict[f'vistEncoder.layers.{i}.feed_forward.conv_1.bias'] = rvc_state_dict[f'enc_p.encoder.ffn_layers.{i}.conv_1.bias']\n",
    "        new_state_dict[f'vistEncoder.layers.{i}.feed_forward.conv_2.weight'] = rvc_state_dict[f'enc_p.encoder.ffn_layers.{i}.conv_2.weight']\n",
    "        new_state_dict[f'vistEncoder.layers.{i}.feed_forward.conv_2.bias'] = rvc_state_dict[f'enc_p.encoder.ffn_layers.{i}.conv_2.bias']\n",
    "\n",
    "    # --- Flow Mapping (flow) ---\n",
    "    print(\"Mapping Flow weights...\")\n",
    "    rvc_flow_indices = [0, 2, 4, 6]\n",
    "    for i in range(n_flows):\n",
    "        rvc_idx = rvc_flow_indices[i]\n",
    "        # FIX: Add '.flows' to the key to match the new model structure\n",
    "        prefix = f'vitsFlow.flows.{i}'\n",
    "        rvc_prefix = f'flow.flows.{rvc_idx}.enc'\n",
    "\n",
    "        new_state_dict[f'{prefix}.conv_pre.weight'] = rvc_state_dict[f'flow.flows.{rvc_idx}.pre.weight']\n",
    "        new_state_dict[f'{prefix}.conv_pre.bias'] = rvc_state_dict[f'flow.flows.{rvc_idx}.pre.bias']\n",
    "        new_state_dict[f'{prefix}.conv_post.weight'] = rvc_state_dict[f'flow.flows.{rvc_idx}.post.weight']\n",
    "        new_state_dict[f'{prefix}.conv_post.bias'] = rvc_state_dict[f'flow.flows.{rvc_idx}.post.bias']\n",
    "        for j in range(n_wavenet_layers):\n",
    "            new_state_dict[f'{prefix}.wavenet.in_layers.{j}.bias'] = rvc_state_dict[f'{rvc_prefix}.in_layers.{j}.bias']\n",
    "            new_state_dict[f'{prefix}.wavenet.in_layers.{j}.parametrizations.weight.original0'] = rvc_state_dict[f'{rvc_prefix}.in_layers.{j}.weight_g']\n",
    "            new_state_dict[f'{prefix}.wavenet.in_layers.{j}.parametrizations.weight.original1'] = rvc_state_dict[f'{rvc_prefix}.in_layers.{j}.weight_v']\n",
    "            new_state_dict[f'{prefix}.wavenet.res_skip_layers.{j}.bias'] = rvc_state_dict[f'{rvc_prefix}.res_skip_layers.{j}.bias']\n",
    "            new_state_dict[f'{prefix}.wavenet.res_skip_layers.{j}.parametrizations.weight.original0'] = rvc_state_dict[f'{rvc_prefix}.res_skip_layers.{j}.weight_g']\n",
    "            new_state_dict[f'{prefix}.wavenet.res_skip_layers.{j}.parametrizations.weight.original1'] = rvc_state_dict[f'{rvc_prefix}.res_skip_layers.{j}.weight_v']\n",
    "        new_state_dict[f'{prefix}.wavenet.cond_layer.bias'] = rvc_state_dict[f'{rvc_prefix}.cond_layer.bias']\n",
    "        new_state_dict[f'{prefix}.wavenet.cond_layer.parametrizations.weight.original0'] = rvc_state_dict[f'{rvc_prefix}.cond_layer.weight_g']\n",
    "        new_state_dict[f'{prefix}.wavenet.cond_layer.parametrizations.weight.original1'] = rvc_state_dict[f'{rvc_prefix}.cond_layer.weight_v']\n",
    "\n",
    "    # --- Other RVC-specific layers ---\n",
    "    print(\"Mapping remaining RVC-specific layers...\")\n",
    "    # FIX: Change 'emb_phone' to 'phoneme_embedding'\n",
    "    new_state_dict['phoneme_embedding.weight'] = rvc_state_dict['enc_p.emb_phone.weight']\n",
    "    new_state_dict['phoneme_embedding.bias'] = rvc_state_dict['enc_p.emb_phone.bias']\n",
    "    new_state_dict['emb_pitch.weight'] = rvc_state_dict['enc_p.emb_pitch.weight']\n",
    "    new_state_dict['emb_g.weight'] = rvc_state_dict['emb_g.weight']\n",
    "    new_state_dict['proj.weight'] = rvc_state_dict['enc_p.proj.weight']\n",
    "    new_state_dict['proj.bias'] = rvc_state_dict['enc_p.proj.bias']\n",
    "\n",
    "    # --- Decoder (dec) ---\n",
    "    print(\"Mapping Decoder weights...\")\n",
    "    for key, value in rvc_state_dict.items():\n",
    "        if key.startswith('dec.'):\n",
    "            new_key = key.replace('dec.', 'hybrid_decoder.')\n",
    "            new_state_dict[new_key] = value\n",
    "\n",
    "    # --- Load the state dict ---\n",
    "    print(\"\\nLoading state dictionary...\")\n",
    "    missing_keys, unexpected_keys = target_model.load_state_dict(new_state_dict, strict=False)\n",
    "\n",
    "    if unexpected_keys:\n",
    "        print(f\"\\nWarning: Unexpected keys in state_dict: {unexpected_keys}\")\n",
    "    if missing_keys:\n",
    "        print(f\"\\nWarning: Missing keys in state_dict: {missing_keys}\")\n",
    "\n",
    "    print(\"\\nWeight transfer complete!\")\n",
    "    return target_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee850e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "config, vitsConfig, weight, output_sampling_rate, savePath, cpt, cptPath = openModel(\"your favorite model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd469406",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = convert_rvc_to_hf_vits(weight, config, vitsConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f05a8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),savePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5b952b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(cptPath, \"w\") as f:\n",
    "    json.dump(cpt, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaada8ae",
   "metadata": {},
   "source": [
    "# export hubert\n",
    "# Please download hubert_base.pt from original repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fc4e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.hubert.convert_hubert_original_pytorch_checkpoint_to_pytorch import  convert_hubert_checkpoint\n",
    "convert_hubert_checkpoint(\"hubert_base.pt\", \"models/hubert_base_hf\", is_finetuned=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
